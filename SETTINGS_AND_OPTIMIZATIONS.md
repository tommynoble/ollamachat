# Ollama Chat - Settings & Optimizations

## Overview
This document contains all the critical settings, optimizations, and configurations that have been applied to the Ollama Chat application to ensure optimal performance and prevent common issues.

---

## üîß Backend Settings (main.js)

### Response Handling
**File:** `/main.js` (lines 483-491)

```javascript
const payload = {
  model: modelToUse,
  messages: messages,
  stream: true,  // Enable streaming like official Ollama app
  options: {
    ...modelConfig,
    num_ctx: 4096, // Increase context window for longer responses
  },
};
```

**Why:**
- `stream: true` - Enables streaming responses (prevents truncation)
- `num_ctx: 4096` - Maximum context window for Ollama (allows longer responses)

### HTTP Timeout
**File:** `/main.js` (line 497)

```javascript
timeout: 300000, // 5 minute timeout for long responses
```

**Why:** Long responses need time to generate. 5 minutes prevents premature connection closure.

### Token Limits
**File:** `/main.js` (lines 307-312, 360-367)

```javascript
const baseConfig = {
  temperature: 0.7,
  top_p: 0.9,
  top_k: 40,
  repeat_penalty: 1.1,
  num_predict: 8192, // Maximum length for complete responses
};

// Llama2 optimizations
if (modelName.toLowerCase().includes('llama2')) {
  const baseConfig = {
    temperature: 0.7,
    top_p: 0.9,
    top_k: 40,
    num_predict: 8192, // Even longer responses - no truncation
  };
}
```

**Why:**
- `num_predict: 8192` - Maximum tokens per response (doubled from 4096)
- Prevents responses from being cut off mid-sentence

### Conversation History
**File:** `/main.js` (lines 464-466)

```javascript
// Add conversation history (keep last 15 exchanges for better context and thinking)
const recentHistory = history.slice(-30); // 15 user + 15 assistant messages for deeper context
messages.push(...recentHistory);
```

**Why:** More history = better context for chain-of-thought reasoning

### Response Logging
**File:** `/main.js` (lines 536-540, 522-524)

```javascript
// Log response size for debugging
console.log(`‚úÖ Response received: ${assistantMessage.length} characters`);
if (assistantMessage.length < 100) {
  console.warn('‚ö†Ô∏è WARNING: Response seems very short!');
}

// Log if this is the last chunk (done: true)
if (parsed.done) {
  console.log(`‚úÖ Stream complete. Total response: ${fullResponse.length} chars`);
}
```

**Why:** Helps identify truncation issues by showing actual response size

---

## üé® Frontend Settings (ChatWindow.tsx)

### User Message Styling
**File:** `/src/components/ChatWindow.tsx` (lines 67-71)

```javascript
className={`px-4 py-2 ${
  message.role === 'user'
    ? 'bg-card text-white rounded-2xl max-w-lg opacity-90'
    : 'max-w-2xl text-foreground rounded-2xl px-6 py-3'
}`}
```

**Settings:**
- User messages: `rounded-2xl` (matches chat box)
- User messages: `opacity-90` (subtle transparency)
- User messages: `bg-card` (matches background)
- User messages: `max-w-lg` (compact width)
- AI messages: `max-w-2xl` (wider for longer text)

### Input Area
**File:** `/src/components/ChatWindow.tsx` (line 318)

```javascript
className="flex-1 px-4 py-2 bg-input border border-border rounded-2xl resize-none focus:outline-none focus:ring-2 focus:ring-ring"
```

**Settings:**
- `rounded-2xl` - Matches message bubbles
- `px-4 py-2` - Compact padding

### Typewriter Effect Speed
**File:** `/src/components/ChatWindow.tsx` (line 59)

```javascript
}, 15) // Adjust speed here (lower = faster)
```

**Why:** 15ms per character = smooth, readable animation

### Message Spacing
**File:** `/src/components/ChatWindow.tsx` (line 65)

```javascript
<div className={`flex ${message.role === 'user' ? 'justify-end' : 'justify-start'} ${message.role === 'user' ? 'mb-2' : 'mb-4'}`}>
```

**Settings:**
- User messages: `mb-2` (compact spacing)
- AI messages: `mb-4` (more breathing room)

---

## üß† System Prompt Settings

**File:** `/main.js` (lines 375-409)

The system prompt includes:
- Deep understanding and critical thinking
- Context awareness
- Clarity and accuracy
- Helpfulness and nuance
- Engagement
- **Chain-of-thought reasoning (internal, not shown)**

**Model-Specific Optimizations:**
- **Deepseek:** Enhanced reasoning, logical analysis
- **Llama2:** Versatile, balanced responses
- **Phi:** Efficient, clear explanations
- **CodeLlama:** Production-ready code

---

## üìä Performance Tuning

### Temperature Settings
- **Deepseek:** 0.5 (focused, fast)
- **Llama2:** 0.7 (balanced)
- **Phi:** 0.8 (creative)
- **CodeLlama:** 0.3 (precise)

### Top-P (Nucleus Sampling)
- **Deepseek:** 0.8 (narrow sampling)
- **Llama2:** 0.9 (balanced)
- **Phi:** 0.95 (creative)

### Top-K (Top-K Sampling)
- **Deepseek:** 30 (fewer options = faster)
- **Llama2:** 40 (balanced)
- **Phi:** 50 (more vocabulary)

---

## üêõ Debugging Checklist

### If Responses Are Truncated:
1. ‚úÖ Check console for: `‚úÖ Stream complete. Total response: XXXX chars`
2. ‚úÖ If < 500 chars: Response is genuinely short
3. ‚úÖ If > 2000 chars: Truncation is UI-side, not backend
4. ‚úÖ Verify `num_predict: 8192` is set
5. ‚úÖ Verify `num_ctx: 4096` is set
6. ‚úÖ Check `stream: true` is enabled

### If App Crashes:
1. ‚úÖ Check for missing imports (e.g., `ImageIcon`)
2. ‚úÖ Verify all lucide-react icons are imported
3. ‚úÖ Check console for TypeScript errors

### If Chat Stops Working:
1. ‚úÖ Verify Ollama is running (green dot in Settings)
2. ‚úÖ Check timeout isn't too short (should be 300000ms)
3. ‚úÖ Verify streaming is enabled

---

## üìù Key Files to Monitor

| File | Purpose | Critical Lines |
|------|---------|-----------------|
| `main.js` | Backend, response handling | 307-312, 360-367, 483-491, 497 |
| `ChatWindow.tsx` | UI, message display | 67-71, 318, 59, 65 |
| `rag_system.py` | Document processing | All |
| `preload.js` | IPC security | All |

---

## üöÄ Deployment Checklist

Before deploying, verify:
- ‚úÖ `num_predict: 8192` in both baseConfig and llama2 config
- ‚úÖ `num_ctx: 4096` in payload options
- ‚úÖ `stream: true` enabled
- ‚úÖ `timeout: 300000` set
- ‚úÖ All imports present (Image, FileText, Send, Paperclip)
- ‚úÖ Message spacing correct (mb-2 for user, mb-4 for AI)
- ‚úÖ Border radius consistent (rounded-2xl)
- ‚úÖ Opacity set for user messages (opacity-90)

---

## üìö References

- **Ollama API Docs:** https://github.com/ollama/ollama/blob/main/docs/api.md
- **ChromaDB:** https://www.trychroma.com/
- **Framer Motion:** https://www.framer.com/motion/
- **Lucide Icons:** https://lucide.dev/

---

---

## üéØ Accuracy & Performance Improvements (Latest)

### Temperature & Sampling (Factual Accuracy)
**File:** `/main.js` (lines 315-380)

Changed from creative defaults to accuracy-focused:
```javascript
const baseConfig = {
  temperature: 0.3,      // Was 0.7 - cooler for factual accuracy
  top_p: 0.85,           // Was 0.9 - narrower sampling
  top_k: 40,
  repeat_penalty: 1.1,
  num_predict: 2048,     // Was 8192 - balanced length
  seed: 42,              // Reproducible outputs for testing
};
```

**Why:** Lower temperature = more deterministic, factual responses. Seed ensures reproducibility.

### Context Window Expansion
**File:** `/main.js` (line 503)

```javascript
num_ctx: 8192,  // Larger context prevents silent truncation
```

**Why:** Prevents important context from being cut off mid-conversation.

### Model-Specific Tuning

**Deepseek-R1 (Reasoning Model):**
```javascript
temperature: 0.3,
num_predict: 2048,
// Removed: stop: ["<think>", "</think>"] - caused API errors
```

**CodeLlama (Code Generation):**
```javascript
temperature: 0.2,  // Very low for precise code
```

**Llama2 (Balanced):**
```javascript
temperature: 0.3,
```

### System Prompt Accuracy Guidelines
**File:** `/main.js` (lines 418-423)

Added explicit accuracy instructions:
```
ACCURACY GUIDELINES:
- When facts are uncertain, say "I'm not sure" and ask for a source
- Prefer concise, specific claims; avoid guessing names, dates, or figures
- If giving a list of steps, ensure each step is actionable and verifiable
- Cite sources when making factual claims
- Say "I don't know" rather than guessing
```

### Keep-Alive & Thinking
**File:** `/main.js` (lines 499-500)

```javascript
keep_alive: "10m",  // Keep model loaded for 10 minutes
think: model.toLowerCase().includes('deepseek'),  // Enable thinking for reasoning models
```

### Timeout
**File:** `/main.js` (line 502)

```javascript
timeout: 600000,  // 10 minutes for long responses
```

---

## üêõ Bug Fix: "No response received" Error

### Problem
After adding accuracy settings, responses returned "No response received" error.

### Root Cause
The `stop: ["<think>", "</think>"]` parameter was not supported by Ollama API and caused the request to fail silently.

### Solution
**Removed the stop parameter** from Deepseek config:
```javascript
// REMOVED - caused API errors:
// stop: ["<think>", "</think>"]
```

### Result
‚úÖ Responses now work correctly with accuracy settings applied
‚úÖ All models respond properly
‚úÖ No more "No response received" errors

---

---

## üé® Markdown Rendering & Professional Formatting (Latest)

### Dependencies Added
**File:** `package.json`

```bash
npm i react-markdown remark-gfm
npm i -D @tailwindcss/typography
```

### Frontend Changes
**File:** `/src/components/ChatWindow.tsx`

#### Helper Function:
```javascript
function closeDanglingFence(s: string) {
  const fences = (s.match(/```/g) || []).length
  return fences % 2 === 1 ? s + '\n```' : s
}
```

#### Message Rendering:
- **User messages:** Instant display (no typewriter)
- **Assistant messages:** Markdown rendering with custom components

#### Custom Markdown Components:
```javascript
components={{
  h2: (p) => <h2 className="text-xl font-bold mt-6 mb-3" {...p} />,
  h3: (p) => <h3 className="text-lg font-semibold mt-4 mb-2" {...p} />,
  ul: (p) => <ul className="list-disc pl-6 space-y-2" {...p} />,
  ol: (p) => <ol className="list-decimal pl-6 space-y-2" {...p} />,
  li: (p) => <li className="leading-relaxed" {...p} />,
  strong: (p) => <strong className="font-semibold" {...p} />,
  p: (p) => <p className="leading-relaxed mb-3" {...p} />,
  code: (p) => <code className="px-1.5 py-0.5 rounded bg-muted/40 font-mono text-sm" {...p} />,
  pre: (p) => <pre className="p-3 rounded bg-muted/40 overflow-x-auto mb-3" {...p} />,
}}
```

### System Prompt Update
**File:** `/main.js` (lines 424-428)

Added FORMAT RULES:
```
FORMAT RULES:
- Always answer in GitHub-flavored Markdown.
- Use numbered section headings (## 1., ## 2., ‚Ä¶).
- For each item, use **bold** labels followed by clear text.
- Use bullet lists where helpful. No HTML.
```

### Tailwind Configuration
**File:** `tailwind.config.js`

```javascript
plugins: [require('@tailwindcss/typography')],
```

### Why This Works:
- ‚úÖ No broken Markdown while streaming
- ‚úÖ Professional formatting for headings, lists, code blocks
- ‚úÖ Syntax highlighting for code
- ‚úÖ Proper typography with prose styles
- ‚úÖ Auto-closes dangling code fences
- ‚úÖ User messages instant, assistant messages smooth

### Result:
- üìù **Headings:** Styled h2/h3 with proper spacing
- üìã **Lists:** Bullet points with proper indentation
- üíª **Code blocks:** Syntax highlighted, auto-closes
- üìñ **Typography:** Professional prose styles
- ‚ú® **Smooth rendering:** No character-by-character breaking

---

**Last Updated:** October 29, 2025  
**Status:** Production Ready ‚úÖ (Markdown + Accuracy Mode Enabled)
